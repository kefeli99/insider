{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insider Trial Day\n",
    "## Personalized Recommendation Model & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(path=\"train.parquet\", engine=\"fastparquet\")\n",
    "\n",
    "# Convert 'date' to datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# Sort by userId and date to ensure chronological order\n",
    "df = df.sort_values(by=[\"date\", \"userId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "# Helper function to safely evaluate items\n",
    "def safe_eval(item):\n",
    "    if item.startswith(\"[\") and item.endswith(\"]\"):\n",
    "        try:\n",
    "            return ast.literal_eval(item)\n",
    "        except:\n",
    "            return [item]\n",
    "    return item\n",
    "\n",
    "\n",
    "def get_items(item_list):\n",
    "    items = []\n",
    "    for item in item_list:\n",
    "        if item != \"[]\":\n",
    "            evaluated_item = safe_eval(item)\n",
    "            if isinstance(evaluated_item, list):\n",
    "                items.extend(evaluated_item)\n",
    "            else:\n",
    "                items.append(evaluated_item)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data by sessionId\n",
    "session_df = df.groupby(['userId', 'sessionId']).agg({\n",
    "    'date': ['min', 'max'],  # Session start and end times\n",
    "    'pageType': lambda x: list(x),  # List of page types visited\n",
    "    'itemId': get_items,\n",
    "    'category': get_items,\n",
    "    'productPrice': ['mean', 'max', 'min'],  # Price statistics\n",
    "    'oldProductPrice': ['mean', 'max', 'min']  # Old price statistics\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "session_df.columns = ['userId', 'sessionId', 'session_start', 'session_end',\n",
    "                      'page_types', 'items', 'categories',\n",
    "                      'avg_price', 'max_price', 'min_price',\n",
    "                      'avg_old_price', 'max_old_price', 'min_old_price']\n",
    "\n",
    "# Fill missing values\n",
    "session_df['avg_price'] = session_df['avg_price'].fillna(0)\n",
    "session_df['min_price'] = session_df['min_price'].fillna(0)\n",
    "session_df['max_price'] = session_df['max_price'].fillna(0)\n",
    "session_df['avg_old_price'] = session_df['avg_old_price'].fillna(0)\n",
    "session_df['min_old_price'] = session_df['min_old_price'].fillna(0)\n",
    "session_df['max_old_price'] = session_df['max_old_price'].fillna(0)\n",
    "\n",
    "# Feature Engineering: Add additional features\n",
    "session_df['session_length'] = (session_df['session_end'] - session_df['session_start']).dt.total_seconds()\n",
    "session_df['num_items'] = session_df['items'].apply(len)\n",
    "session_df['num_categories'] = session_df['categories'].apply(len)\n",
    "session_df['num_page_types'] = session_df['page_types'].apply(len)\n",
    "session_df['purchase'] = session_df['page_types'].apply(lambda x: 1 if 'success' in x else 0)\n",
    "\n",
    "# Extract hour of the day\n",
    "session_df['hour_of_day'] = session_df['session_start'].dt.hour\n",
    "\n",
    "# Extract day of the week\n",
    "session_df['day_of_week'] = session_df['session_start'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of sessions per user\n",
    "user_session_count = session_df.groupby('userId')['sessionId'].nunique().rename('total_sessions')\n",
    "session_df = session_df.merge(user_session_count, on='userId', how='left')\n",
    "\n",
    "# Calculate total number of purchases per user\n",
    "user_purchase_count = session_df.groupby('userId')['purchase'].sum().rename('total_purchases')\n",
    "session_df = session_df.merge(user_purchase_count, on='userId', how='left')\n",
    "\n",
    "# Calculate average session duration per user\n",
    "user_avg_session_duration = session_df.groupby('userId')['session_length'].mean().rename('avg_session_duration')\n",
    "session_df = session_df.merge(user_avg_session_duration, on='userId', how='left')\n",
    "\n",
    "# Ensure sessions are sorted by userId and session_start\n",
    "session_df = session_df.sort_values(by=['userId', 'session_start'])\n",
    "\n",
    "# Calculate recency (days since last session)\n",
    "session_df['last_session'] = session_df.groupby('userId')['session_end'].shift(1)\n",
    "session_df['recency'] = (session_df['session_start'] - session_df['last_session']).dt.total_seconds() / (60 * 60 * 24)\n",
    "session_df['recency'] = session_df['recency'].fillna(session_df['recency'].max())  # Fill NaNs with max recency\n",
    "\n",
    "# Drop the temporary 'last_session' column\n",
    "session_df = session_df.drop(columns=['last_session'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of categories per user and count occurrences\n",
    "user_preferred_categories = session_df.explode('categories').groupby(['userId', 'categories']).size().unstack(fill_value=0)\n",
    "user_top_categories = user_preferred_categories.apply(lambda x: x.sort_values(ascending=False).index[:3].tolist(), axis=1).rename('top_categories')\n",
    "\n",
    "# Add the top categories to the session dataframe\n",
    "session_df = session_df.merge(user_top_categories, on='userId', how='left')\n",
    "\n",
    "# Similarly, for items\n",
    "user_preferred_items = session_df.explode('items').groupby(['userId', 'items']).size().unstack(fill_value=0)\n",
    "user_top_items = user_preferred_items.apply(lambda x: x.sort_values(ascending=False).index[:3].tolist(), axis=1).rename('top_items')\n",
    "\n",
    "# Add the top items to the session dataframe\n",
    "session_df = session_df.merge(user_top_items, on='userId', how='left')\n",
    "\n",
    "# Fill missing values in 'top_items' and 'top_categories' with empty lists\n",
    "session_df['top_items'] = session_df['top_items'].fillna('').apply(list)\n",
    "session_df['top_categories'] = session_df['top_categories'].fillna('').apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightFM \n",
    "\n",
    "1. Hybrid Approach:\n",
    "   LightFM is a hybrid model that combines collaborative filtering with content-based filtering.\n",
    "   - It can leverage both user-item interactions  and item features.\n",
    "   - This hybrid approach helps mitigate the cold-start problem for new items or users, as it can make recommendations based on item features even when there's no interaction data.\n",
    "\n",
    "2. Scalability:\n",
    "   LightFM is built to handle large-scale datasets efficiently. It uses stochastic gradient descent for optimization, which allows it to scale to large numbers of users and items.\n",
    "\n",
    "3. Handling Sparse Data:\n",
    "   E-commerce datasets are often very sparse (users interact with only a tiny fraction of available items). LightFM is designed to handle this sparsity well.\n",
    "\n",
    "4. Feature Incorporation:\n",
    "   The ability to incorporate item features (in our case, categories) allows the model to understand similarities between items beyond just user interactions. This can lead to more diverse recommendations.\n",
    "\n",
    "\n",
    "#### Alternatives Considered:\n",
    "- Simple collaborative filtering methods (like user-user or item-item similarity) were deemed too simplistic for the complexity of e-commerce data.\n",
    "- Matrix Factorization techniques (like SVD) don't easily incorporate item features.\n",
    "- Deep learning models (like neural collaborative filtering) were considered potentially overkill for the dataset size and would require more computational resources.\n",
    "\n",
    "#### Potential Limitations:\n",
    "- If the dataset is extremely large, even LightFM might struggle, and we might need to consider distributed computing solutions.\n",
    "- If we need real-time updates to the model, we might need to consider online learning approaches, which LightFM doesn't natively support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doguk/miniconda3/lib/python3.11/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Flatten items and categories\n",
    "item_data = []\n",
    "for _, row in df.iterrows():\n",
    "    user_id = row[\"userId\"]\n",
    "    for item, cat in zip(row[\"items\"], row[\"categories\"]):\n",
    "        if item != \"[]\" and cat != \"[]\":\n",
    "            # Handle potential nested lists in categories\n",
    "            if isinstance(cat, list):\n",
    "                for c in cat:\n",
    "                    if c != \"[]\":\n",
    "                        item_data.append((user_id, item, c))\n",
    "            else:\n",
    "                item_data.append((user_id, item, cat))\n",
    "\n",
    "item_df = pd.DataFrame(item_data, columns=[\"user_id\", \"item_id\", \"category\"])\n",
    "\n",
    "# Encode categories\n",
    "le = LabelEncoder()\n",
    "item_df[\"category\"] = le.fit_transform(item_df[\"category\"].apply(str))\n",
    "\n",
    "# Create item features\n",
    "item_features = pd.get_dummies(item_df[\"category\"], prefix=\"cat\")\n",
    "item_features[\"item_id\"] = item_df[\"item_id\"]\n",
    "item_features = item_features.groupby(\"item_id\").sum().reset_index()\n",
    "\n",
    "# Create user features\n",
    "# user_features = pd.get_dummies(df[\"top_categories\"].explode(), prefix=\"cat\")\n",
    "# user_features = df.drop(columns=[\"sessionId\", \"session_start\", \"session_end\", \"page_types\", \"items\", \"categories\"])  # Drop unnecessary columns)\n",
    "user_features = df.drop(columns=[\"sessionId\", \"session_start\", \"session_end\", \"page_types\", \"items\", \"categories\", \"top_items\", \"top_categories\"])  # Drop unnecessary columns\n",
    "#rename userId to user_id\n",
    "user_features.rename(columns={'userId':'user_id'}, inplace=True)\n",
    "# drop users with num_items = 0, num_categories = 0\n",
    "user_features = user_features[user_features['num_items'] > 0]\n",
    "user_features = user_features[user_features['num_categories'] > 0]\n",
    "\n",
    "# 3. Create LightFM Dataset\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    item_df[\"user_id\"],\n",
    "    item_df[\"item_id\"], \n",
    "    user_features=user_features.columns[1:],\n",
    "    item_features=item_features.columns[1:],\n",
    ")\n",
    "\n",
    "# Create interaction matrix\n",
    "interactions, weights = dataset.build_interactions(\n",
    "    (row[\"user_id\"], row[\"item_id\"]) for _, row in item_df.iterrows()\n",
    ")\n",
    "\n",
    "# Create user features matrix\n",
    "user_features_matrix = dataset.build_user_features(\n",
    "    (row[\"user_id\"], row.iloc[1:].to_dict()) for _, row in user_features.iterrows()\n",
    ")\n",
    "\n",
    "# Create item features matrix\n",
    "item_features_matrix = dataset.build_item_features(\n",
    "    (row[\"item_id\"], row.iloc[1:].to_dict()) for _, row in item_features.iterrows()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x3d1cad790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Train the Model\n",
    "model = LightFM(\n",
    "    no_components=30,\n",
    "    learning_rate=0.05,\n",
    "    loss=\"warp\",\n",
    "    random_state=42,\n",
    ")\n",
    "model.fit(\n",
    "    interactions,\n",
    "    user_features=user_features_matrix,\n",
    "    item_features=item_features_matrix,\n",
    "    epochs=50,\n",
    "    num_threads=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Generate Recommendations\n",
    "def get_recommendations(\n",
    "    user_id, model, dataset, user_features_matrix, item_features_matrix, n=5\n",
    "):\n",
    "    n_users, n_items = dataset.interactions_shape()\n",
    "\n",
    "    user_id_map = dataset.mapping()[0]\n",
    "    item_id_map = dataset.mapping()[2]\n",
    "    # Create a reverse mapping of internal item indices to item IDs\n",
    "    reverse_item_map = {v: k for k, v in item_id_map.items()}\n",
    "\n",
    "    if user_id not in user_id_map:\n",
    "        return []\n",
    "\n",
    "    user_idx = user_id_map[user_id]\n",
    "\n",
    "    scores = model.predict(\n",
    "        user_idx,\n",
    "        np.arange(n_items),\n",
    "        item_features=item_features_matrix,\n",
    "        user_features=user_features_matrix,\n",
    "    )\n",
    "    top_items = np.argsort(-scores)\n",
    "\n",
    "    return [reverse_item_map[item] for item in top_items[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 0001d86ea81e6eef12cebaa1dcbdadc2:\n",
      "['e24b3c0c71eac81c49867fda76bcd1a3', '126b7550606fcc57843c50adf557f482', '4fcb83d905354249b964950d8fb3a2e3', 'e21e83f213fff4d98aa3df84e088ec1b', 'c78fb70476792ceacefd511d0abb4c5c']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_id = item_df[\"user_id\"].iloc[0]\n",
    "\n",
    "recommendations = get_recommendations(user_id, model, dataset,user_features_matrix, item_features_matrix)\n",
    "print(f\"Recommendations for user {user_id}:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision@10: 0.0273\n",
      "Train AUC: 0.9146\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from lightfm.evaluation import precision_at_k, auc_score\n",
    "\n",
    "train_precision = precision_at_k(\n",
    "    model,\n",
    "    interactions,\n",
    "    user_features=user_features_matrix,\n",
    "    item_features=item_features_matrix,\n",
    "    k=10,\n",
    ").mean()\n",
    "\n",
    "train_auc = auc_score(\n",
    "    model,\n",
    "    interactions,\n",
    "    user_features=user_features_matrix,\n",
    "    item_features=item_features_matrix,\n",
    ").mean()\n",
    "\n",
    "print(f\"Train Precision@10: {train_precision:.4f}\")\n",
    "\n",
    "print(f\"Train AUC: {train_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_artifacts/user_features.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'model_artifacts/lightfm_model.joblib')\n",
    "joblib.dump(dataset, 'model_artifacts/lightfm_dataset.joblib')\n",
    "joblib.dump(item_features_matrix, 'model_artifacts/item_features_matrix.joblib')\n",
    "joblib.dump(item_features, 'model_artifacts/item_features.joblib')\n",
    "joblib.dump(user_features_matrix, 'model_artifacts/user_features_matrix.joblib')\n",
    "joblib.dump(user_features, 'model_artifacts/user_features.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train Precision@10: 0.0273 (2.73%)\n",
    "2. Train AUC: 0.9146 (91.46%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Precision@10 (0.0273):\n",
    "   This score suggests that, on average, 2.73% of the top 10 recommended items for each user are relevant (i.e., items that the user has actually interacted with). At first glance, this might seem low, but it's important to consider a few factors:\n",
    "   - In recommendation systems, especially with large item catalogs, even seemingly low precision scores can be valuable.\n",
    "   - The score depends on the sparsity of the data. If users typically interact with only a small fraction of available items, a lower precision is expected.\n",
    "\n",
    "#### 2. AUC (0.9146):\n",
    "   This is actually a good score. AUC (Area Under the ROC Curve) of 0.9146 indicates that the model is very good at distinguishing between items a user is likely to interact with and those they're not. A perfect AUC would be 1.0, so 0.9146 suggests strong predictive power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
